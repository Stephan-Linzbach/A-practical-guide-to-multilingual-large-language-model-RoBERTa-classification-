{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stephan-Linzbach/RoBERTaTutorial/blob/main/TextClassificaiton_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KuYVJkLn_b0",
        "outputId": "0dd55f64-6b57-4725-ceab-e6ef3e0d45be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.32.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FiSAWk_HKqT",
        "outputId": "04114012-58ba-4cf4-8f8b-a0185c258a40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-08-22 14:42:38--  https://raw.githubusercontent.com/davda54/sam/main/sam.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2484 (2.4K) [text/plain]\n",
            "Saving to: ‘sam.py’\n",
            "\n",
            "\rsam.py                0%[                    ]       0  --.-KB/s               \rsam.py              100%[===================>]   2.43K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-22 14:42:38 (41.9 MB/s) - ‘sam.py’ saved [2484/2484]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/davda54/sam/main/sam.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-ZpvTl_jHGEi"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader\n",
        "from sam import SAM\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DGVQ7lJTloP9"
      },
      "outputs": [],
      "source": [
        "## Utils\n",
        "\n",
        "def flatten_list(list_to_flatten):\n",
        "  return [x for xs in list_to_flatten for x in xs]\n",
        "\n",
        "def infer_output_size(data):\n",
        "  labels = data['Labels']\n",
        "  if isinstance(labels[0], list):\n",
        "    labels = flatten_list(labels)\n",
        "  labels = set(labels)\n",
        "  return len(labels), labels\n",
        "\n",
        "def convert_in_output_size(labels, mapping):\n",
        "  label_resized = []\n",
        "  for l in labels:\n",
        "    tmp_l = torch.tensor([1 if k in l else 0 for m in mapping])\n",
        "    label_resized.append(tmp_l)\n",
        "  label_resized = torch.stack(label_resized, dim=0)\n",
        "  return label_resized\n",
        "\n",
        "def convert_labels(labels, mapping):\n",
        "  if isinstance(labels[0], list):\n",
        "    labels = convert_in_output_size(labels, mapping)\n",
        "  return torch.tensor([mapping[l] for l in labels])\n",
        "\n",
        "def generate_dataloader(text, y, batch_size, workers=1):\n",
        "    attention_mask = text['attention_mask']\n",
        "    input_ids = text['input_ids']\n",
        "\n",
        "    dataset = list(zip(input_ids, attention_mask, y))\n",
        "\n",
        "    dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=workers)\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abq2zMSNIs6y"
      },
      "source": [
        "## Text Classification\n",
        "\n",
        "After this tutorial you are able to use state-of-the-art tools to train a model with which you can classify your own textual data. \\\\\n",
        "Today we will look at how to **fine-tune** a **pre-trained language model** for a custom **text-classification**. \\\\\n",
        "You might be asking yourself two things:\n",
        "  - What exactly is a text-classification?\n",
        "  - What is a pre-trained language model?\n",
        "  - What even is fine-tuning?\n",
        "\n",
        "No worries we will answer this questions one by one as we work our way through this tutorial.\n",
        "\n",
        "Starting probably with the most pressing question: What exactly is a text-classification?\n",
        "In text-classification we try to assign a property to a text. \\\\\n",
        "\n",
        "For example we are interested in classifying texts that are about fruits.\n",
        "We could easily find a dictionary with all fruits (e.g.: 'Apple', 'Banana', 'Pear' etc.) everytime we recognize such a word in a text we know this text is about fruits, right?\n",
        "However, this might not be true all the time for example \"My Apple pencil is not working.\" is not about the fruit 'Apple' although we would recognize it as such with our dictionary approach. \\\\\n",
        "So the context of the word might be helpful (more on this later). \\\\\n",
        "Classification is obviously transferable to more than just fruits. \\\\\n",
        "People try to classify the sentiment of a text, the stance towards an entity expressed in a text, the topic of a text, the expressed emotion in a text, and many many more. \\\\\n",
        "\n",
        "Let's talk data:\n",
        "In order to make this script work you have to save three dictionaries in this structure in the file '<current_folder>/(train|val|test).json': \\\\\n",
        "  ``{'Text' : [list of texts]\n",
        "     'Labels': [list of labels]}`` \\\\\n",
        "\n",
        "  ``length([list of texts]) equals length([list of texts])`` \\\\\n",
        "\n",
        "Example \\\\\n",
        " ``{'Text': ['Yesterday i ate an apple.', 'Yesterday I crashed my Apple.'],\n",
        "     'Labels': ['about_fruit', 'not_about_fruit']}``\n",
        "\n",
        "The train data is used to teach the model, the val data is required to test if the model understands the train data correctly and the test data is used to proof the capabilities of the final version of the model on the **unseen** test set\n",
        "\n",
        "You can have one label per text (multi-class classification) or several labels per text (multi-label classification). \\\\\n",
        "\n",
        "Is your data ready? Then lets start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lRaIwoOMIrtX",
        "outputId": "ea5417e6-18b6-40b1-d820-172a0c5931c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We loaded the train data with 192 texts and 192 labels,\n",
            "the validation data with 192 texts and 192 labels\n",
            "and the test data with 192 texts and 192 labels.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  with open(\"./train.json\") as f:\n",
        "    train = json.load(f)\n",
        "\n",
        "  with open(\"./val.json\") as f:\n",
        "    val = json.load(f)\n",
        "\n",
        "  with open(\"./test.json\") as f:\n",
        "    test = json.load(f)\n",
        "except:\n",
        "  #Dummy Values\n",
        "  train = {'Text': [\"Number of texts does not match number of labels for train data!\", \"Number of texts does not match number of labels for val data!\", \"Number of texts does not match number of labels for test data!\"]*64,\n",
        "          'Labels': ['is_correct', 'is_incorrect', 'is_incorrect']*64}\n",
        "  val = {'Text': [\"Number of texts does not match number of labels for train data!\", \"Number of texts does not match number of labels for val data!\", \"Number of texts does not match number of labels for test data!\"]*64,\n",
        "          'Labels': ['is_incorrect', 'is_correct', 'is_incorrect']*64}\n",
        "  test = {'Text': [\"Number of texts does not match number of labels for train data!\", \"Number of texts does not match number of labels for val data!\", \"Number of texts does not match number of labels for test data!\"]*64,\n",
        "          'Labels': ['is_incorrect', 'is_incorrect', 'is_correct']*64}\n",
        "\n",
        "\n",
        "assert len(train['Text']) == len(train['Labels']), \"Number of texts does not match number of labels for train data!\"\n",
        "assert len(val['Text']) == len(val['Labels']), \"Number of texts does not match number of labels for val data!\"\n",
        "assert len(test['Text']) == len(test['Labels']), \"Number of texts does not match number of labels for test data!\"\n",
        "\n",
        "print(f\"We loaded the train data with {len(train['Text'])} texts and {len(train['Labels'])} labels,\")\n",
        "print(f\"the validation data with {len(val['Text'])} texts and {len(val['Labels'])} labels\")\n",
        "print(f\"and the test data with {len(test['Text'])} texts and {len(test['Labels'])} labels.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BYdtu44Xrhp"
      },
      "source": [
        "Great the data is ready!\n",
        "\n",
        "You have to answer some questions about your data.\n",
        "\n",
        "In which language is your text data written?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHzEmeSnXq2w"
      },
      "outputs": [],
      "source": [
        "language = input(\"Which language do you use? ('english', 'german', 'french') \")\n",
        "\n",
        "print(f\"MMhhh interesting your data is written in {language}. Let's load the correct LLM!\")\n",
        "\n",
        "if language == 'english':\n",
        "  model_name = \"roberta-base\"\n",
        "elif language == 'german':\n",
        "  model_name = \"benjamin/roberta-base-wechsel-german\"\n",
        "elif language == 'french':\n",
        "  model_name = \"camembert-base\"\n",
        "else:\n",
        "  print(f\"Seems like we have no model available for this {language}.\")\n",
        "  print(\"We will load a mulitlingual language model. It knows 100 text from 100 languages.\")\n",
        "  model_name = 'xlm-roberta-base'\n",
        "\n",
        "print(f\"We loaded {model_name} for {language}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq68uB4cbnKc"
      },
      "source": [
        "Ok now that we talked about the language of your data you might be intersted what the ``model_name`` stands for. \\\\\n",
        "These are **pre-trained language models** ready to be used with your specific language.\n",
        "These language models already learned to understand language by solving a huge cloze-text written in the particular language. \\\\\n",
        "This cloze-text is constructed over Wikipedia or other huge text datasets.\n",
        "\n",
        "Now that we understood what **text classification** and **pre-trained language model** are.\n",
        "\n",
        "We can now talk about the last question: What is **fine-tuning**?\n",
        "\n",
        "As you might imagine solving a cloze-text over the whole internet makes you knowledgeable but not an expert in a field. We now want to transform our **pre-trained language model** into an expert for your task.\n",
        "\n",
        "So lets start to understand your task.\n",
        "\n",
        "We differentiate two classification tasks:\n",
        "  - Choosing (What is your favorite fruit?) ('single_label_classification')\n",
        "    - Choose one of the possible labels as the correct label.\n",
        "  - Deciding (Do you like apples?) ('multi_label_classification')\n",
        "    - Decide for each possible label if it is a correct label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP2b9HkBY1Qc",
        "outputId": "5dcdca9c-d43f-43e6-959d-042d93b9f9f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do you want to classify by choosing or by deciding: choosing\n"
          ]
        }
      ],
      "source": [
        "decision_type = input(\"Do you want to classify by choosing or by deciding: \")\n",
        "\n",
        "assert decision_type in ['deciding', 'choosing']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFBY_wgsg6wg"
      },
      "source": [
        "Based on your ``decision_type`` we will now choose the correct loss function and the decision function.\n",
        "The loss function tells the model how well it achieved your task.\n",
        "The decision function tells us how to convert the model guesses in the actual decision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZD7XiARg6KS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "losses = {'deciding': \"multi_label_classification\", #torch.nn.CrossEntropyLoss(),\n",
        "          'choosing': \"single_label_classification\",}\n",
        "\n",
        "decisions = {'deciding': lambda x: torch.where(x > 0, 1, 0),\n",
        "             'choosing': lambda x: torch.argmax(x, dim=1)}\n",
        "\n",
        "objective = losses[decision_type]\n",
        "decision_function = decisions[decision_type]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZcFgWRZiM8o"
      },
      "source": [
        "The next question we need to clarify is: How many different labels are possible for your task?\n",
        "\n",
        "Example: \\\\\n",
        "Choose your favorite fruit from this list: \\\\\n",
        "  ``poss_labels = ['Banana', 'Apple', 'Pear', 'Peach'`]`` \\\\\n",
        "  ``model_output = [0.4, 0.5, -0.1, 0.7]`` \\\\\n",
        "\n",
        "Or decide if you like the particular fruit: \\\\\n",
        "  ``poss_labels = ['Banana', 'Apple', 'Pear', 'Peach'`]`` \\\\\n",
        "  ``model_output = [0.4, 0.5, -0.1, 0.7]`` \\\\\n",
        "In both cases we have **4** possible labels.\n",
        "In the first case we choose the fruit where the model signals the biggest aggreement ``'Peach'`` in the second we decide for each fruit if we like it by chosing to like everything above 0. Therefore, our example output tells us that we like ``['Banana', 'Apple', 'Peach']``.\n",
        "\n",
        "Let's find out how many labels are possible in our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6AKMt2JiLd7",
        "outputId": "b706f80d-2a33-4a16-8f16-71556d85b95e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How many different labels are possible for your task? 2\n",
            "Your task distinguishes 2 different labels.\n"
          ]
        }
      ],
      "source": [
        "output_size = input(\"How many different labels are possible for your task? \")\n",
        "infered_output_size_train, possible_labels_train = infer_output_size(train)\n",
        "infered_output_size_val, possible_labels_val = infer_output_size(val)\n",
        "infered_output_size_test, possible_labels_test = infer_output_size(test)\n",
        "\n",
        "possible_labels = max(infered_output_size_train, infered_output_size_val, infered_output_size_test)\n",
        "\n",
        "\n",
        "if not (possible_labels_train == possible_labels_val == possible_labels_test):\n",
        "  print(f\"Train contains {possible_labels_train} possible labels, val contains {possible_labels_val} possible labels and test contains {possible_labels_test} possible labels.\"\\\n",
        "        \"This might be unwanted and is not recommended all sets should contain all possible labels.\")\n",
        "\n",
        "assert int(output_size), \"Output_size needs to be a natural number.\"\n",
        "assert int(output_size) == possible_labels, f\"We infered {possible_labels} with the following possible labels {possible_labels_train}.\"\n",
        "assert possible_labels_train == possible_labels_val == possible_labels_test, f\"Make sure that train, val and test labels are equal!\"\n",
        "\n",
        "id2label = {i: k for i,k in enumerate(possible_labels_train)}\n",
        "label2id = {k: i for i, k in enumerate(possible_labels_train)}\n",
        "\n",
        "output_size = int(output_size)\n",
        "\n",
        "print(f\"Your task distinguishes {output_size} different labels. These are {possible_labels_train}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYAu-jSLtSBf"
      },
      "source": [
        "Fantastic!\n",
        "\n",
        "We are close.\n",
        "We clarified the language, the objective and number of possible answers. \\\\\n",
        "Lets load the needed model and tokenizer.\n",
        "The tokenizer translates language into a model specific vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGtsc2tGtQWi"
      },
      "outputs": [],
      "source": [
        "model_config = {'pretrained_model_name_or_path': model_name,\n",
        "                'num_labels': output_size,\n",
        "                'problem_type': objective,\n",
        "                'id2label': id2label,\n",
        "                'label2id': label2id}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDYrCWQwHBS4",
        "outputId": "86cdfcbd-3d9c-4211-a57d-b4d07e570800"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(**model_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9WzBdSRo5LT"
      },
      "source": [
        "\n",
        "With the now loaded model and tokenizer we can start **fine-tuning**. \\\\\n",
        "\n",
        "For now we only need the train-set to teach the model and the val-set to decide if we taught our model well.\n",
        "Lets first translate our text into the models vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIP_Jj34oz9r"
      },
      "outputs": [],
      "source": [
        "train_text = tokenizer([m for m in train['Text']], truncation=True, padding='longest', return_tensors='pt')\n",
        "val_text = tokenizer([m for m in val['Text']], truncation=True, padding='longest', return_tensors='pt')\n",
        "test_text = tokenizer([m for m in test['Text']], truncation=True, padding='longest', return_tensors='pt')\n",
        "\n",
        "\n",
        "train_y = convert_labels(train['Labels'], label2id)\n",
        "val_y = convert_labels(val['Labels'], label2id)\n",
        "test_y = convert_labels(test['Labels'], label2id)\n",
        "\n",
        "train_dataloader = generate_dataloader(train_text, train_y, batch_size)\n",
        "val_dataloader = generate_dataloader(val_text, val_y, batch_size)\n",
        "test_dataloader = generate_dataloader(test_text, test_y, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLFf-h69n8Nn"
      },
      "source": [
        "Now we need to specify some training specific things. Dont worry if you dont know what to change the pre-set values should already be working just fine.\n",
        "Lets get first an intuition how training of a model actually works. You show a model some example data points in your case the train data. From this data the model infers helpful patterns that explain the correlation between input and output. In order to get the most out of our data we show a small amount of data each time (batch_size) and let the model learn from it. To protect the model from overestimating its understanding we restrict the learned patterns per batch by applying the learning rate (lr). We also apply a warm-up (warm-up-rate) to ensure that the model doesnt forget everything it knew from the pre-training. After all we are using **pre-trained language models** for a reason.\n",
        "\n",
        " Lets go through each of the values one by one:\n",
        "  - the batch_size tells us: How many examples we show to the model before we deduce rules that help to solve the classification task.\n",
        "  - the learning_rate (lr) tells us: How hard we want the model to commit on the recognized patterns from the batch\n",
        "  - the num_epoch tells us: How often we show all the training data to the model in this case 3 times.\n",
        "  - the warm_up_rate tells us: How big of a share of the training should be less impactful.\n",
        "  - device tells us: What device are we using if you have special hardware available (e.g. GPU) your training runs much faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWLgh6LRGNOE"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "lr = 1e-4\n",
        "num_epochs = 3\n",
        "warm_up_rate = 0.1\n",
        "num_training_steps = (len(train['Labels'])//batch_size)*num_epochs\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELylkOMArO-h"
      },
      "source": [
        "The learning of patterns and adaptation of the model is achieved by the optimizer. In our case it is a special optimizer that keeps a model from optimizing. If you are really interested you can read more about it [here](https://github.com/davda54/sam). The scheduler adapts the learning rate according the warm_up_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFuqFXajGfJ6"
      },
      "outputs": [],
      "source": [
        "optimizer = SAM(model.parameters(), torch.optim.Adam, lr=lr, adaptive=True)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer = optimizer,\n",
        "                                          num_warmup_steps = num_training_steps*warm_up_rate,\n",
        "                                          num_training_steps = num_training_steps,\n",
        "                                          last_epoch = -1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_hiMENYrwM0"
      },
      "source": [
        "## Training\n",
        "\n",
        "Lets start the training!\n",
        "I wont go into too much detail but now we show the data to the model and optimize its parameters to succeed in the task as good as possible.\n",
        "Each epoch we test our model, we always safe the best model.\n",
        "Once this is done you should have a well trained model that you can load with the code of the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e35OhfF0JO52",
        "outputId": "0e34b5b5-5d2b-4057-87f6-c6d24462b8d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: Epoch 0, Train step 0, Loss 0.7241153717041016, learning_rate 0.0\n",
            "Validation: Epoch 0, Train step 0, Loss 0.7246332764625549, old best/epoch nf/0\n",
            "**** END EPOCH 0 ****\n",
            "Train: Epoch 1, Train step 0, Loss 0.7241153717041016, learning_rate 0.0001\n",
            "Validation: Epoch 1, Train step 0, Loss 0.6579864025115967, old best/epoch ensor/0\n",
            "**** END EPOCH 1 ****\n",
            "Train: Epoch 2, Train step 0, Loss 0.6574143767356873, learning_rate 0.0\n",
            "Validation: Epoch 2, Train step 0, Loss 0.6579864621162415, old best/epoch ensor/1\n",
            "**** END EPOCH 2 ****\n",
            "**** FINISHED TRAINING FOR N=2 ****\n",
            "BEST EPOCH: 1\n",
            "BEST LOSS: 0.6579864025115967\n"
          ]
        }
      ],
      "source": [
        "best_loss = float('inf')\n",
        "best_epoch = 0\n",
        "already_trained = 0\n",
        "best_model_path = ''\n",
        "should_delete = True\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  for batch_idx, batch in enumerate(train_dataloader):\n",
        "    input_ids, attention_mask, y = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "\n",
        "    output = model(input_ids, attention_mask, labels=y)\n",
        "    loss = output.loss\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    scheduler.step()\n",
        "    optimizer.first_step(zero_grad=True)\n",
        "\n",
        "    model(input_ids, attention_mask, labels=y).loss.backward()\n",
        "    optimizer.second_step(zero_grad=True)\n",
        "    print(f\"Train: Epoch {epoch}, Train step {already_trained+batch_idx}, Loss {loss}, learning_rate {scheduler.get_last_lr()[0]}\", flush=True)\n",
        "\n",
        "  already_trained += batch_idx\n",
        "  val_loss = []\n",
        "  for batch_idx, batch in enumerate(val_dataloader):\n",
        "    input_ids, attention_mask, y = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      val_loss.append(model(input_ids, attention_mask, labels=y).loss)\n",
        "\n",
        "    val_loss = torch.mean(torch.stack(val_loss))\n",
        "\n",
        "    print(f\"Validation: Epoch {epoch}, Train step {already_trained}, Loss {val_loss}, old best/epoch {str(best_loss)[1:6]}/{best_epoch}\", flush=True)\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "      best_loss = val_loss\n",
        "      best_epoch = epoch\n",
        "      if should_delete and best_model_path:\n",
        "        shutil.rmtree(best_model_path)\n",
        "      best_model_path = f\"./my_model_epoch_{best_epoch}_val_loss_{str(val_loss.item())[1:6]}\"\n",
        "      model.save_pretrained(best_model_path, from_pt=True)\n",
        "\n",
        "    print(f\"**** END EPOCH {epoch} ****\")\n",
        "\n",
        "\n",
        "print(f\"**** FINISHED TRAINING FOR N={epoch} ****\")\n",
        "print(f\"BEST EPOCH: {best_epoch}\")\n",
        "print(f\"BEST LOSS: {best_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6IoeYsasTlt"
      },
      "source": [
        "The training is finished now we can load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlEDLy1qCONM"
      },
      "outputs": [],
      "source": [
        "best_model = AutoModelForSequenceClassification.from_pretrained(best_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N8bn_AVsXY7"
      },
      "source": [
        "Finally, with the loaded model we can now predict our results for the unseen test set to understand the models performance in more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF0nHp1aRaHc"
      },
      "outputs": [],
      "source": [
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  for batch_idx, batch in enumerate(test_dataloader):\n",
        "    input_ids, attention_mask, y = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      y_pred += model(input_ids, attention_mask, labels=y).logits\n",
        "\n",
        "\n",
        "y_pred = torch.stack(y_pred, dim=0)\n",
        "y_pred = decision_function(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXp0j0r5Tg4y",
        "outputId": "110c79ff-2c1c-4dfa-8f77-b60e115c9610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "  is_correct       1.00      0.00      0.00         1\n",
            "is_incorrect       0.67      1.00      0.80         2\n",
            "\n",
            "    accuracy                           0.67         3\n",
            "   macro avg       0.83      0.50      0.40         3\n",
            "weighted avg       0.78      0.67      0.53         3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(test_y, y_pred, target_names=label2id.keys(), zero_division=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwpXy9G1lhHD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPC0zktnBUeTrkHGYvVZiH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}